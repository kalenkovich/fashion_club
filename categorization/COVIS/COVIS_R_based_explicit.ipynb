{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RuleBasedSystem(object):\n",
    "    class Rule(object):\n",
    "        def __init__(self, c, sigma_e_2, dim_ind, delta_c, salience):\n",
    "            self.c = c  # decision criterion\n",
    "            self.sigma_e_2 = sigma_e_2  # variance of the noise added during making a decision\n",
    "            self.dim_ind = dim_ind  # the index of the dimenstion this rule is concerned with\n",
    "            self.delta_c = delta_c  # for updating the criterion\n",
    "            self.salience = salience  # current salience\n",
    "        \n",
    "        def get_discriminant_value(self, x):\n",
    "            \"\"\"\n",
    "            Page 68, Eq. (1)\n",
    "            h_E(x) = x_i - C_i\n",
    "            \"\"\"\n",
    "            return x[self.dim_ind] - self.c\n",
    "        \n",
    "        def make_decision (self, x):\n",
    "            \"\"\"\n",
    "            The decision is \"B\" if the discriminant value is larger than random noise and \"A\" otherwise.\n",
    "            Page 68:\n",
    "            \n",
    "            \"Respond A on trial n if h_E(x) < ε; respond B if h_E(x) > ε\"\n",
    "            \n",
    "            There are only strict inequalities here. The probability of equality is near zero but it still\n",
    "            cleaner to stick equality somewhere.\n",
    "            \n",
    "            Note: For some reason, larger values on any dimension are associated with a higher probabilty of\n",
    "            selecting \"B\".\n",
    "            \"\"\"\n",
    "            h_E = self.get_discriminant_value(x)  # h_E(x)\n",
    "            epsilon = np.random.normal(scale = np.sqrt(self.sigma_e_2))\n",
    "            if h_E > epsilon:\n",
    "                return \"B\"\n",
    "            elif h_E <= epsilon:  \n",
    "                return \"A\"\n",
    "            \n",
    "        def update_salience(self, delta):\n",
    "            \"\"\"\n",
    "            Change salience by delta\n",
    "            \"\"\"\n",
    "            self.salience += delta \n",
    "            self.salience = max(0, self.salience)  # to avoid negative salience values\n",
    "            \n",
    "        def update(self, x, feedback):\n",
    "            \"\"\"\n",
    "            We could not understand from the book whether we should compare the discriminant value to the noise\n",
    "            during the criterion update. For now we decided that using noise here does not make much sense.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Determine what decision we would lean towards if there was no noise\n",
    "            h_E = self.get_discriminant_value(x)\n",
    "            if h_E > 0:\n",
    "                deterministic_decision = 'B'\n",
    "            elif h_E <= 0:\n",
    "                deterministic_decision = 'A'\n",
    "            \n",
    "            # Larger c => greater chance of selecting 'A'\n",
    "            # We update the criterion only if there is a mistake. The change is constnant for a given rule - self.delta_c\n",
    "            if deterministic_decision == 'B' and feedback == 'A':\n",
    "                self.c += self.delta_c\n",
    "            elif deterministic_decision == 'A' and feedback == 'B':\n",
    "                self.c -= self.delta_c\n",
    "                \n",
    "        def __str__(self):\n",
    "            return 'One-dimensional rule on dimension {} with C={}'.format(\n",
    "                self.dim_ind, self.c)\n",
    "                \n",
    "            \n",
    "    def __init__(self, n_dims, sigma_e_2, gamma, lambda_, delta_C, delta_E, delta_criterion):\n",
    "        \n",
    "        self.n_dims = n_dims  # number of dimensions (r) \n",
    "        self.sigma_e_2 =  sigma_e_2  # variance of the noise added during making a decision by each rule\n",
    "        self.gamma = gamma  # perceverance, lower=easier_switch\n",
    "        self.lambda_ = lambda_  #selection_param, =mu for Poisson for X, \n",
    "                                # higher lambdo=higher prob in n+1\n",
    "        # These values are used to update the salience of the selected rule\n",
    "        self.delta_C = delta_C  # in case the decision was correct\n",
    "        self.delta_E = delta_E  # in case the decision led to an error\n",
    "        \n",
    "        # This value is used to update rules' criteria\n",
    "        self.delta_criterion = delta_criterion\n",
    "        \n",
    "        c_init = 0.5  # unless the de\n",
    "        salience_init = 1.0 / n_dims\n",
    "        \n",
    "        self.rules = [self.Rule(c=c_init, \n",
    "                                sigma_e_2=sigma_e_2, \n",
    "                                dim_ind=dim_ind, \n",
    "                                delta_c=delta_criterion, \n",
    "                                salience=salience_init) \n",
    "                      for dim_ind in range(n_dims)]\n",
    "        \n",
    "        # Confidence\n",
    "        self.confidence_in_prediction = None\n",
    "        self.max_h_E = 0\n",
    "        \n",
    "        # During each iteration two rules get temporary salience bumps - the last one used and a random one.\n",
    "        # Since 'the last one used' does not make sense on the first iteration, let's just pick one at random.\n",
    "        self.current_rule = np.random.choice(self.rules, 1)[0]\n",
    "        \n",
    "    def process_stimulus(self, x, real_category=None):\n",
    "        self.current_rule = self._select_rule()\n",
    "        self._update_confidence(self.current_rule.get_discriminant_value(x))\n",
    "        self.last_stimulus = x\n",
    "        self.current_prediction = self.current_rule.make_decision(x)  # current response\n",
    "        if real_category is not None:\n",
    "            is_correct = self.process_feedback(real_category)\n",
    "            return is_correct\n",
    "        \n",
    "    def _select_rule(self):\n",
    "        saliences = [rule.salience for rule in self.rules]\n",
    "        \n",
    "        # Perseveration, Y_i(n) = Z_i(n) + gamma, p. 69, Eq. (4)\n",
    "        current_rule_ind = self.rules.index(self.current_rule)\n",
    "        saliences[current_rule_ind] += self.gamma\n",
    "        \n",
    "        # Select a random rule, Rj\n",
    "        n_rules = len(self.rules)\n",
    "        indices = list(range(n_rules))\n",
    "        indices.remove(current_rule_ind) #remove Ri\n",
    "        random_rule_ind = np.random.choice(indices, 1)[0] #select Rj\n",
    "        \n",
    "        # Add X to the random rule's salience, Y_j(n)=Z_j(n) + X, p. 70, Eq. (5)\n",
    "        X = np.random.poisson(lam=self.lambda_)\n",
    "        saliences[random_rule_ind] += X\n",
    "        \n",
    "        # Select a rule randomly, p(Rk)=Yk(n)/sum(Ysal(n)) sum_sal: 1 to m, p. 70, Eq. (7)\n",
    "        pties = [salience / sum(saliences) for salience in saliences]\n",
    "        return np.random.choice(self.rules, size=1, p=pties)[0]\n",
    "        \n",
    "    def process_feedback(self, feedback):\n",
    "        is_correct = self.current_prediction == feedback\n",
    "        self._update_saliences(feedback)\n",
    "        self._update_rules(feedback)\n",
    "        \n",
    "        return is_correct\n",
    "    \n",
    "    def _update_saliences(self, feedback):\n",
    "        # The salience is updated only for the selected rule. Depending on the success of the prediction,\n",
    "        # it is changed by one of the two constants. P. 69, Eqs. (2), (3)\n",
    "        if self.current_prediction == feedback:\n",
    "            delta = self.delta_C\n",
    "        else:\n",
    "            delta = -self.delta_E\n",
    "        self.current_rule.update_salience(delta=delta)\n",
    "        \n",
    "    def _update_rules(self, feedback):\n",
    "        x = self.last_stimulus\n",
    "        for rule in self.rules:\n",
    "            rule.update(x, feedback)\n",
    "            \n",
    "    def _update_confidence(self, discriminant_value):\n",
    "        # p. 77\n",
    "        h_E = abs(discriminant_value)\n",
    "        \n",
    "        # Normalize by the historical maximum value\n",
    "        if h_E > self.max_h_E:\n",
    "            self.max_h_E = h_E\n",
    "            self.confidence_in_prediction = h_E\n",
    "        else:\n",
    "            self.confidence_in_prediction = h_E / self.max_h_E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fashion]",
   "language": "python",
   "name": "conda-env-fashion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
